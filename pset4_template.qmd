---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

import packages
```{python}
import pandas as pd
import numpy as np
import altair as alt
import os
import json
alt.renderers.enable("png") 
import shapely
import geopandas as gpd
```

1. 
```{python}
raw_health = r"/Users/justinesilverstein/Desktop/problem-set-4-andrew-justine"

#raw_health = r"C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine"
path_health16 = os.path.join(raw_health, "pos2016.csv")
health16 = pd.read_csv(path_health16) 

path_health17 = os.path.join(raw_health, "pos2017.csv")
health17 = pd.read_csv(path_health17)

path_health18 = os.path.join(raw_health, "pos2018.csv")
health18 = pd.read_csv(path_health18, encoding = "ISO-8859-1")

path_health19 = os.path.join(raw_health, "pos2019.csv")
health19 = pd.read_csv(path_health19, encoding = "ISO-8859-1")
```

Attribution: I asked ChatGPT how to solve the unicode errors I 
was encountering, and I was advised to try out different
encodings, including "ISO-8859-1".

Q1 Answer:
```{python}
variables_list = health16.columns

print("These are the variables I selected: ", variables_list)
```


2. 
Function for subsetting the datasets to short term hospitals
```{python}
def short_term(X):
  Y = X[(X["PRVDR_CTGRY_SBTYP_CD"] == 1) & (X["PRVDR_CTGRY_CD"] == 1)]
  return(Y)

```

Run function on health16, creating a count of 
```{python}
short16 = short_term(health16)

print("There are ", len(short16), " hospitals in this dataset")
```

    a.
    b.
3. 

short term function on health17
```{python}
short17 = short_term(health17)

```

short term function on health18
```{python}
short18 = short_term(health18)

```

```{python}
short19 = short_term(health19)

```

Appending the data together
```{python}
# add a "year" column to each dataset, for future disagg
def add_column(X, the_year):
  year_col = []
  i = 0
  while i < len(X):
    year_col.append(the_year)
    i += 1
  X["Year"] = year_col
  return(X)

#run on each dataframe
add_column(short16, 2016)

add_column(short17, 2017)

add_column(short18, 2018)

add_column(short19, 2019)
```

Attribution: I asked my code why my len(X) and length of index
values were not equal, and was advised to set i equal to 0 
instead of 1. 

Concat the dataframes together
```{python}
concat_1 = pd.concat([short16, short17], axis = 0)

concat_2 = pd.concat([concat_1, short18], axis = 0)

df_health = pd.concat([concat_2, short19], axis = 0)

df_health.reset_index(drop = True)
```

Attribution: https://datacarpentry.org/python-ecology-lesson/05-merging-data.html 

Code 
```{python}
def  one_dummy(C):
  observation = []
  i = 0
  #produces a 1 for every entry
  while 1 > 0 and i < C:
    observation.append(1)
    i += 1
  return(observation)

#add observation column to df
df_health["Observation"] = one_dummy(len(df_health))


```

Attribution: modifying my own code from Pset 1
Graph observations by year
```{python}
alt.data_transformers.enable("vegafusion")

alt.Chart(df_health).mark_bar().encode(
  alt.X("Year:N"),
  alt.Y("sum(Observation)")
)


```

4.

```{python}
# create a function to iterate through PRVDR_NUM 
def unique_comp(column):
  unique_box = []
  for entry in column:
    if entry not in unique_box:
      unique_box.append(entry)
    else:
      pass
  return(unique_box)

#save list
unique_list = unique_comp(df_health["PRVDR_NUM"])

```

Filter by year in order to make numbers for each year, save those numbers to a df
```{python}
# make series with the four years
the_years = [2016, 2017, 2018, 2019]

#2016
unique_len_16 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2016]))

#2017
unique_len_17 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2017]))

#2018
unique_len_18 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2018]))

#2019
unique_len_19 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2019]))

# make the lens into a list
len_list = [unique_len_16, unique_len_17, unique_len_18, unique_len_19]

#form into dataframe

the_data = {
  "Years": the_years,
   "Unique_CMS": len_list 
}

df_CMS = pd.DataFrame(the_data)

```

Plot the graph
```{python}
alt.Chart(df_CMS).mark_bar().encode(
  alt.X("Years:N"),
  alt.Y("Unique_CMS")
)


```

Attribution: https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/ 
    a.
    b.

## Identify hospital closures in POS file (15 pts) (*)

1. Create a list of all hospitals that were active in 2016 that were suspected to have closed by 2019. Record the facility name and zip of each hospital as well as the year of suspected closure. How many hospitals fit in this definition?

3,303 hospitals fit this definition, out of 29085 total hospitals.

```{python}
active_2016 = df_health[(df_health['PGM_TRMNTN_CD'] == 0) & (df_health['Year'] == 2016)]

def find_suspected_closures(active_2016, df_health):
    suspected_closures = pd.DataFrame(columns=['FAC_NAME', 'ZIP_CD', 'Year'])
    
    for index, row in active_2016.iterrows():
        FAC_NAME = row['FAC_NAME']
        ZIP_CD = row['ZIP_CD']
        found = False  
        for Year in range(2017, 2020):
            closed_check = df_health[(df_health['FAC_NAME'] == FAC_NAME) & (df_health['Year'] == Year)]  
            if not closed_check.empty and closed_check['PGM_TRMNTN_CD'].values[0] != 0:
                new_entry = pd.DataFrame({
                    'FAC_NAME': [FAC_NAME],
                    'ZIP_CD': [ZIP_CD],
                    'Year': [Year]
                })
                suspected_closures = pd.concat([suspected_closures, new_entry], ignore_index=True)
                found = True 

        if not found:
            for Year in range(2017, 2020):
                new_entry = pd.DataFrame({
                    'FAC_NAME': [FAC_NAME],
                    'ZIP_CD': [ZIP_CD],
                    'Year': [Year]
                })
                suspected_closures = pd.concat([suspected_closures, new_entry], ignore_index=True)

    return suspected_closures

suspected_closures = find_suspected_closures(active_2016, df_health)
print(len(suspected_closures.loc[suspected_closures.groupby('FAC_NAME')['Year'].idxmin()]))
```

2. Sort this list of hospitals by name and report the names and year of suspected closure for the first 10 rows.

```{python}
print(suspected_closures.loc[suspected_closures.groupby('FAC_NAME')['Year'].idxmin()].sort_values('FAC_NAME')[['FAC_NAME', 'Year']].head(10))
```

3. Not all closures are true closures, remove any suspected closures that are zipcodes where the number does not decrease in the year after the suspected closure.

    a. Among the suspected closures, how many hospitals fit this definition? After correcting, how many do you have left?

    Working with the definition given, we have 166 mergers/acquisitions.

```{python}
zip_counts = suspected_closures.groupby(['ZIP_CD', 'Year']).size().reset_index(name='Observation_Count')

# Step 2: Sort by ZIP_CD and Year
zip_counts = zip_counts.sort_values(['ZIP_CD', 'Year'])

# Step 3: Calculate the change in observations
zip_counts['Prev_Observation_Count'] = zip_counts.groupby('ZIP_CD')['Observation_Count'].shift(1)
zip_counts['Change'] = zip_counts['Observation_Count'] - zip_counts['Prev_Observation_Count']

# Step 4: Filter for ZIP codes with no decrease or no disappearance
no_decrease = zip_counts[(zip_counts['Change'] >= 0) | (zip_counts['Prev_Observation_Count'].notnull())]

# Step 5: Select relevant columns and display results
result = no_decrease[['ZIP_CD', 'Year', 'Observation_Count']]
print(result)
```

```{python}
# Merge the DataFrames on ZIP_CD
merged_results = pd.merge(suspected_closures, no_decrease, on='ZIP_CD', how='inner')

# Drop duplicates to show only unique FAC_NAMEs
unique_fac_names = merged_results[['FAC_NAME']].drop_duplicates()

# Display the unique FAC_NAMEs
print(unique_fac_names)
```

```{python}
print(suspected_closures[['FAC_NAME', 'Year']].sort_values(by='FAC_NAME').head(10)) how ask only for unique fac_name in the first year they appear?

filtered_codes = suspected_closures.groupby(['ZIP_CD', 'Year']) and list the number of obersations we have per zip code and year
```

    b. How many hospitals do you have left?
      Subtracting the mergers, we have 10084 hospitals left.

```{python}
print((len(suspected_closures) - len(filtered_closures)))
```

    c. Sort and report the first 10 rows.

```{python}
print(filtered_closures.sort_values(by='FAC_NAME').head(10))
``` 

## Download Census zip code shapefile (10 pt) 



1. 
    a. The five file types in this download are .dbf files, .prj files,
     .shp files, .shx files, and .xml file. .dbf Files have attribute information,
     .shp files have feature geometrics, and .shx files contain a positional index,
     while .prj files describe the Coordinate Reference System and .xml files 
     are used to store data in hierarchical categories. 

     Attribution: in-class lecture and https://blog.hubspot.com/website/what-is-xml-file 

    b. In terms of uncompressed
     sizes, the largest files are the .shp and .dbf files, at 817,915 and 6,275 KB, 
     followed by the .shx and .xml files at 259 and 16 KB. Coming in last is the 
     .prj file at 1 KB.




1. 
    a.
    b. 
2. 

Read the data into GeoDataFrame
```{python}
#read the .shp file in
the_path = "C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine/gz_2010_us_860_00_500k.shp"

shape_data = gpd.read_file(the_path)

```

Attribution: https://automating-gis-processes.github.io/CSC/notebooks/L2/geopandas-basics.html 

Restrict shape_data only to Texas zip codes

First create a list of Texas zipcodes
```{python}

#generate list of Texas zipcode leading numbers
Tex_zip_list = np.linspace(start = 750, stop = 799, num = 50).astype(np.int32)
#turn to list object
Tex_zip_list = list(Tex_zip_list)

#add Austin entry to Tex_zip_list
Tex_zip_list.append(733)

string_Tex_list = str(Tex_zip_list)
```

This function is used only for checking
zipcodes
```{python}
#make a function to pull out the first 3 letters in the 
#string for each entry 
def string_puller(df):
  #a receiver for the values
  my_strings = []
  #subset for the relevant column
  A = df["ZCTA5"]
  for i in A:
    #select the first 3 characters in each zipcode
    my_strings.append(i[0:3])
  return(my_strings)
```

Create a column for being in Texas
```{python}
#create a binary variable for in or outside of Texas
def in_Texas(df, condition):
  Texas = []
  for i in df[condition]:
    #select the first 3 characters in each zipcode
    #if they have a match in the zip code list
    #they are in Texas
    if i[0:3] in string_Tex_list:
      Texas.append(1)
    else:
      Texas.append(0)
  df["in_Texas"] = Texas
  return(df)
    
#run function on shape_data
in_Texas(shape_data, "ZCTA5")
```

Restrict shape_data to in_Texas
```{python}
Tex_shape = shape_data[shape_data["in_Texas"] == 1]

```

Attribution: https://stackoverflow.com/questions/35928170/is-there-a-numpy-function-for-generating-sequences-similar-to-rs-seq-function 

Calculate the number of hospitals by zipcode in 2016

Clean up ZIP_CD data type
```{python}
#convert ZIP_CD to float
df_health["ZIP_CD"] = df_health["ZIP_CD"].astype(float)
#convert ZIP_CD to int
df_health["ZIP_CD"] = df_health["ZIP_CD"].apply(np.int64)
#convert ZIP_CD to string
df_health["ZIP_CD"] = df_health["ZIP_CD"].astype(str)
```

Use function from earlier to subset df_health to in_Texas
```{python}
#add in_Texas column to df_health
in_Texas(df_health, "ZIP_CD")

#subset df_health to in_Texas == 1
Tex_health = df_health[df_health["in_Texas"] == 1]
```

Created a grouped object containing
```{python}

def by_zip(df):
  #subset for 2016
  hosp_2016 = df[df["Year"] == 2016]

  #groupby zipcode, select CMS Identification number, and 
  #return a count of the CMS ID numbers associated with
  #each Zipcode
  grouped = hosp_2016.groupby("ZIP_CD")["PRVDR_NUM"].count()

  #reset index
  grouped = grouped.reset_index()

  #save to dataframe
  df_grouped = pd.DataFrame(grouped)
  return(df_grouped)

#run on Tex_health
CMS_by_zip = by_zip(Tex_health)

print(CMS_by_zip)
```

Attribution: https://realpython.com/pandas-groupby/ "Example 1".

Add the CMS Number column from the grouped df to 
the shape data using pd.merge()
```{python}

Texas_hospitals = pd.merge(Tex_shape, CMS_by_zip, left_on = "ZCTA5", right_on = "ZIP_CD", how = "inner")


```

Attribution: https://www.statology.org/pandas-merge-on-different-column-names/
I also asked ChatGPT how to use the code suggested in the article above. 


## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 

```{python}
import geopandas as gpd

# Load the GeoDataFrame for ZIP code centroids (assumed to be already done)
# zips_all_centroids = gpd.read_file('path_to_zip_code_data.shp') # If not already loaded

# Filter for Texas ZIP codes (assuming Texas ZIP codes start with '75')
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZIP_CD'].str.startswith('75')]

# Display the number of unique ZIP codes in Texas
unique_texas_zip_codes = zips_texas_centroids['ZIP_CD'].nunique()
print(f"Unique ZIP codes in Texas: {unique_texas_zip_codes}")

# Create a mask for bordering states (Texas and bordering states)
bordering_states_prefixes = ['75', '87', '73', '72', '70']
zips_texas_borderstates_centroids = zips_all_centroids[
    zips_all_centroids['ZIP_CD'].str.startswith(tuple(bordering_states_prefixes))
]

# Display the number of unique ZIP codes in Texas and bordering states
unique_bordering_zip_codes = zips_texas_borderstates_centroids['ZIP_CD'].nunique()
print(f"Unique ZIP codes in Texas and bordering states: {unique_bordering_zip_codes}")

```

2. 

```{python}
# Sample structure of hospital data
hospital_data = {
    'ZIP_CD': ['75001', '75002', '75003'],  # Example ZIP codes
    'Hospitals_2016': [1, 0, 3]              # Number of hospitals in 2016
}
hospitals_df = pd.DataFrame(hospital_data)

```

3. 

```{python}
import geopandas as gpd

# Assuming zips_texas_centroids and zips_withhospital_centroids are already created

# Ensure both GeoDataFrames are in the same coordinate reference system (CRS)
zips_texas_centroids = zips_texas_centroids.to_crs(zips_withhospital_centroids.crs)

# Calculate distance to the nearest hospital ZIP code for each Texas ZIP code
def calculate_nearest_hospital_distance(texas_zips, hospital_zips):
    distances = []
    for _, tex_zip in texas_zips.iterrows():
        # Calculate distance to all hospital ZIP codes
        distance_series = hospital_zips.geometry.distance(tex_zip.geometry)
        # Get the minimum distance
        min_distance = distance_series.min()
        distances.append(min_distance)
    return distances

# Add the distance to the nearest hospital to the Texas ZIP codes GeoDataFrame
zips_texas_centroids['Nearest_Hospital_Distance'] = calculate_nearest_hospital_distance(zips_texas_centroids, zips_withhospital_centroids)

# Display the result
print(zips_texas_centroids[['ZIP_CD', 'Nearest_Hospital_Distance']])

```

4. 

```{python}
import geopandas as gpd

# Assuming zips_texas_centroids and zips_withhospital_centroids are already created

# Ensure both GeoDataFrames are in the same coordinate reference system (CRS)
zips_texas_centroids = zips_texas_centroids.to_crs(zips_withhospital_centroids.crs)

# Calculate distance to the nearest hospital ZIP code for each Texas ZIP code
def calculate_nearest_hospital_distance(texas_zips, hospital_zips):
    distances = []
    for _, tex_zip in texas_zips.iterrows():
        # Calculate distance to all hospital ZIP codes
        distance_series = hospital_zips.geometry.distance(tex_zip.geometry)
        # Get the minimum distance
        min_distance = distance_series.min()
        distances.append(min_distance)
    return distances

# Add the distance to the nearest hospital to the Texas ZIP codes GeoDataFrame
zips_texas_centroids['Nearest_Hospital_Distance'] = calculate_nearest_hospital_distance(zips_texas_centroids, zips_withhospital_centroids)

# Display the result
print(zips_texas_centroids[['ZIP_CD', 'Nearest_Hospital_Distance']])


```

    a.
    b.
5. 

```{python}
# Calculate the average distance to the nearest hospital
average_distance = zips_texas_centroids['Nearest_Hospital_Distance'].mean()

# Display the average distance
print(f"Average distance to the nearest hospital for each ZIP code in Texas: {average_distance:.2f} units")

```

    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
