---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_JS_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_1_\*\* Late coins left after submission: \*\*\_2_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

import packages
```{python}
import pandas as pd
import numpy as np
import altair as alt
import os
import json
alt.renderers.enable("png") 
import shapely
import geopandas as gpd
```

1. 
```{python}
#raw_health = r"/Users/justinesilverstein/Desktop/problem-set-4-andrew-justine"

raw_health = r"C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine"
path_health16 = os.path.join(raw_health, "pos2016.csv")
health16 = pd.read_csv(path_health16) 

path_health17 = os.path.join(raw_health, "pos2017.csv")
health17 = pd.read_csv(path_health17)

path_health18 = os.path.join(raw_health, "pos2018.csv")
health18 = pd.read_csv(path_health18, encoding = "ISO-8859-1")

path_health19 = os.path.join(raw_health, "pos2019.csv")
health19 = pd.read_csv(path_health19, encoding = "ISO-8859-1")
```

Attribution: I asked ChatGPT how to solve the unicode errors I 
was encountering, and I was advised to try out different
encodings, including "ISO-8859-1".

Q1 Answer:
```{python}
variables_list = health16.columns

print("These are the variables I selected: ", variables_list)
```


2. 
Function for subsetting the datasets to short term hospitals
```{python}
def short_term(X):
  Y = X[(X["PRVDR_CTGRY_SBTYP_CD"] == 1) & (X["PRVDR_CTGRY_CD"] == 1)]
  return(Y)

```

Run function on health16, creating a count of hospitals
```{python}
short16 = short_term(health16)

print("There are ", len(short16), " hospitals in this dataset")
```

    a. There are 7,245 short term 
       hospitals in the 2016 dataset. This number seems
       a bit low.
    b. Upon re-reading some of the article provided in this pset and taking 
       a look at data from Statista (https://www.statista.com/statistics/185843/number-of-all-hospitals-in-the-us/)
       I see that this number appears to be, in fact, a bit high. The
       article provided in the dataset places the number of short term,
       acute care hospitals at around 5000, while in the Statology 
       explainer the total number of U.S. hospitals in 2016 is pegged
       at around 5,500. Some potential causes for this discrepancy may include
       varying definitions of hospital, and particularly short term hospital;
       the fact that, potentially, the data from Statista and the Kaiser 
       Family Foundation consists of hospital numbers as they stood at a
       different time than the time in which the data were recorded for this
       dataset (e.g.: this dataset covers Q4, perhaps other sources 
       gather data for Q1? Or average over an entire year.)
3. 

short term function on health17
```{python}
short17 = short_term(health17)

```

short term function on health18
```{python}
short18 = short_term(health18)

```

```{python}
short19 = short_term(health19)

```

Appending the data together
```{python}
# add a "year" column to each dataset, for future disagg
def add_column(X, the_year):
  year_col = []
  i = 0
  while i < len(X):
    year_col.append(the_year)
    i += 1
  X["Year"] = year_col
  return(X)

#run on each dataframe
add_column(short16, 2016)

add_column(short17, 2017)

add_column(short18, 2018)

add_column(short19, 2019)
```

Attribution: I asked my code why my len(X) and length of index
values were not equal, and was advised to set i equal to 0 
instead of 1. 

Concat the dataframes together
```{python}
concat_1 = pd.concat([short16, short17], axis = 0)

concat_2 = pd.concat([concat_1, short18], axis = 0)

df_health = pd.concat([concat_2, short19], axis = 0)

df_health.reset_index(drop = True)
```

Attribution: https://datacarpentry.org/python-ecology-lesson/05-merging-data.html 

Code 
```{python}
def  one_dummy(C):
  observation = []
  i = 0
  #produces a 1 for every entry
  while 1 > 0 and i < C:
    observation.append(1)
    i += 1
  return(observation)

#add observation column to df
df_health["Observation"] = one_dummy(len(df_health))


```

Attribution: modifying my own code from Pset 1
Graph observations by year
```{python}
alt.data_transformers.enable("vegafusion")

alt.Chart(df_health).mark_bar().encode(
  alt.X("Year:N"),
  alt.Y("sum(Observation)")
)


```

4.
  a.
```{python}
# create a function to iterate through PRVDR_NUM 
def unique_comp(column):
  unique_box = []
  for entry in column:
    if entry not in unique_box:
      unique_box.append(entry)
    else:
      pass
  return(unique_box)

#save list
unique_list = unique_comp(df_health["PRVDR_NUM"])

```

Filter by year in order to make numbers for each year, save those numbers to a df
```{python}
# make series with the four years
the_years = [2016, 2017, 2018, 2019]

#2016
unique_len_16 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2016]))

#2017
unique_len_17 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2017]))

#2018
unique_len_18 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2018]))

#2019
unique_len_19 = len(unique_comp(df_health["PRVDR_NUM"][df_health["Year"] == 2019]))

# make the lens into a list
len_list = [unique_len_16, unique_len_17, unique_len_18, unique_len_19]

#form into dataframe

the_data = {
  "Years": the_years,
   "Unique_CMS": len_list 
}

df_CMS = pd.DataFrame(the_data)

```

Plot the graph
```{python}
alt.Chart(df_CMS).mark_bar().encode(
  alt.X("Years:N"),
  alt.Y("Unique_CMS")
)


```

Attribution: https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/ 
    b. These plots suggest that, disaggregated by year, each observation
        is equal to a unique hospital. In addition, the number of unique 
        hospitals in 2019 is exactly the same as the number of unique 
        hospitals for the dataset overall, with each year appearing to
        have all unique observations, indicating that either more hospitals
        are being opened than the number that are closing down, or, perhaps
        more likely, that there is an issue with the way we are analyzing
        this data.

## Identify hospital closures in POS file (15 pts) (*)

1. Create a list of all hospitals that were active in 2016 that were suspected to have closed by 2019. Record the facility name and zip of each hospital as well as the year of suspected closure. How many hospitals fit in this definition?

3,130 hospitals fit this definition, out of over 7000 total hospitals.

```{python}
active_2016 = df_health[(df_health['PGM_TRMNTN_CD'] == 0) & (df_health['Year'] == 2016)]

def find_suspected_closures(active_2016, df_health):
    suspected_closures = []
    active_facilities = set(active_2016['FAC_NAME'])

    for year in range(2017, 2020):
        closed_facilities = df_health[(df_health['Year'] == year) & (df_health['PGM_TRMNTN_CD'] != 0)]

        for facility in active_facilities:
            if facility not in closed_facilities['FAC_NAME'].values:
                suspected_closures.append({
                    'FAC_NAME': facility,
                    'ZIP_CD': active_2016.loc[active_2016['FAC_NAME'] == facility, 'ZIP_CD'].values[0],
                    'Year': year
                })
    return pd.DataFrame(suspected_closures)

suspected_closures = find_suspected_closures(active_2016, df_health)
print(len(suspected_closures.loc[suspected_closures.groupby('FAC_NAME')['Year'].idxmin()]))
```

2. Sort this list of hospitals by name and report the names and year of suspected closure for the first 10 rows.

```{python}
print(suspected_closures.loc[suspected_closures.groupby('FAC_NAME')['Year'].idxmin()].sort_values('FAC_NAME')[['FAC_NAME', 'Year']].head(10))
```

3. Not all closures are true closures, remove any suspected closures that are zipcodes where the number does not decrease in the year after the suspected closure.

    a. Among the suspected closures, how many hospitals fit this definition? After correcting, how many do you have left?

    Working with the definition given, we have 3090 mergers/acquisitions.

```{python}
total_closures = suspected_closures.loc[suspected_closures.groupby('FAC_NAME')['Year'].idxmin()].sort_values('FAC_NAME')[['FAC_NAME', 'Year']]
zip_counts = suspected_closures.groupby(['ZIP_CD', 'Year']).size().reset_index(name='Observation_Count')

zip_counts = zip_counts.sort_values(['ZIP_CD', 'Year'])

zip_counts['Prev_Observation_Count'] = zip_counts.groupby('ZIP_CD')['Observation_Count'].shift(1)
zip_counts['Change'] = zip_counts['Observation_Count'] - zip_counts['Prev_Observation_Count']

#Filter for ZIP codes with no decrease or no disappearance
no_decrease = zip_counts[(zip_counts['Change'] >= 0) | (zip_counts['Prev_Observation_Count'].notnull())]

result = no_decrease[['ZIP_CD', 'Year', 'Observation_Count']]
print(result)

merged_results = pd.merge(suspected_closures, no_decrease, on='ZIP_CD', how='inner')
mergers = merged_results[['FAC_NAME']].drop_duplicates()
print(len(mergers))
```

    b. How many hospitals do you have left?
      Subtracting the mergers, we have 40 hospitals left.

```{python}
print((len(total_closures) - len(mergers)))
```

    c. Sort and report the first 10 rows.

```{python}
print(mergers.sort_values(by='FAC_NAME').head(10))
``` 

## Download Census zip code shapefile (10 pt) 

1. 
    a. The five file types in this download are .dbf files, .prj files,
     .shp files, .shx files, and .xml file. .dbf Files have attribute information,
     .shp files have feature geometrics, and .shx files contain a positional index,
     while .prj files describe the Coordinate Reference System and .xml files 
     are used to store data in hierarchical categories. 

     Attribution: in-class lecture and https://blog.hubspot.com/website/what-is-xml-file 

    b. In terms of uncompressed
     sizes, the largest files are the .shp and .dbf files, at 817,915 and 6,275 KB, 
     followed by the .shx and .xml files at 259 and 16 KB. Coming in last is the 
     .prj file at 1 KB.


1. 
    a.
    b. 
2. 

Read the data into GeoDataFrame
```{python}
#read the .shp file in
the_path = "C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine/gz_2010_us_860_00_500k.shp"

#the_path = "/Users/justinesilverstein/Desktop/problem-set-4-andrew-justine/gz_2010_us_860_00_500k.shp"

shape_data = gpd.read_file(the_path)

```

Attribution: https://automating-gis-processes.github.io/CSC/notebooks/L2/geopandas-basics.html 

Restrict shape_data only to Texas zip codes

First create a list of Texas zipcodes
```{python}

#generate list of Texas zipcode leading numbers
Tex_zip_list = np.linspace(start = 750, stop = 799, num = 50).astype(np.int32)
#turn to list object
Tex_zip_list = list(Tex_zip_list)

#add Austin entry to Tex_zip_list
Tex_zip_list.append(733)

string_Tex_list = str(Tex_zip_list)
```

This function is used only for checking
zipcodes
```{python}
#make a function to pull out the first 3 letters in the 
#string for each entry 
def string_puller(df):
  #a receiver for the values
  my_strings = []
  #subset for the relevant column
  A = df["ZCTA5"]
  for i in A:
    #select the first 3 characters in each zipcode
    my_strings.append(i[0:3])
  return(my_strings)
```

Create a column for being in Texas
```{python}
#create a binary variable for in or outside of Texas
def in_Texas(df, condition):
  Texas = []
  for i in df[condition]:
    #select the first 3 characters in each zipcode
    #if they have a match in the zip code list
    #they are in Texas
    if i[0:3] in string_Tex_list:
      Texas.append(1)
    else:
      Texas.append(0)
  df["in_Texas"] = Texas
  return(df)
    
#run function on shape_data
in_Texas(shape_data, "ZCTA5")
```

Restrict shape_data to in_Texas
```{python}
Tex_shape = shape_data[shape_data["in_Texas"] == 1]

```

Attribution: https://stackoverflow.com/questions/35928170/is-there-a-numpy-function-for-generating-sequences-similar-to-rs-seq-function 

Calculate the number of hospitals by zipcode in 2016

Clean up ZIP_CD data type
```{python}
#convert ZIP_CD to float
df_health["ZIP_CD"] = df_health["ZIP_CD"].astype(float)
#convert ZIP_CD to int
df_health["ZIP_CD"] = df_health["ZIP_CD"].apply(np.int64)
#convert ZIP_CD to string
df_health["ZIP_CD"] = df_health["ZIP_CD"].astype(str)
```

Use function from earlier to subset df_health to in_Texas
```{python}
#add in_Texas column to df_health
in_Texas(df_health, "ZIP_CD")

#subset df_health to in_Texas == 1
Tex_health = df_health[df_health["in_Texas"] == 1]
```

Created a grouped object containing
```{python}

def by_zip(df):
  #subset for 2016
  hosp_2016 = df[df["Year"] == 2016]

  #groupby zipcode, select CMS Identification number, and 
  #return a count of the CMS ID numbers associated with
  #each Zipcode
  grouped = hosp_2016.groupby("ZIP_CD")["PRVDR_NUM"].count()

  #reset index
  grouped = grouped.reset_index()

  #save to dataframe
  df_grouped = pd.DataFrame(grouped)
  return(df_grouped)

#run on Tex_health
CMS_by_zip = by_zip(Tex_health)

print(CMS_by_zip)
```


Attribution: https://realpython.com/pandas-groupby/ "Example 1".

Add the CMS Number column from the grouped df to 
the shape data using pd.merge(). Then make choropleth
```{python}
#add an int version of ZIP_CD to CMS_by_zip
CMS_by_zip["Int_Zip"] = [int(i) for i in CMS_by_zip["ZIP_CD"]]

#convert that int version into a string
CMS_by_zip["String_Zip"] = [str(i) for i in CMS_by_zip["Int_Zip"]]

#comprehension where if ZCTA5 in String_Zip add entry into
#new column 
new_col = [i for i in shape_data["ZCTA5"].values if i in CMS_by_zip["String_Zip"].values]

newer_col = [CMS_row["PRVDR_NUM"] for index, CMS_row in CMS_by_zip.iterrows() if CMS_row["String_Zip"] in new_col]



#subsetting for shape_file ZCTA5 == String_Zip in CMS_by_zip

Texas_hospitals = pd.merge(Tex_shape, CMS_by_zip, left_on = "ZCTA5", right_on = "ZIP_CD", how = "left")

```

Attribution: https://www.statology.org/pandas-merge-on-different-column-names/
I also asked ChatGPT how to use the code suggested in the article above. 


## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. Create a GeoDataFrame for the centroid of each zip code nationally:
zips_all_centroids. What are the dimensions of the resulting GeoDataFrame and what do each of the columns mean?

The dimensions of the dataframe are 33120 rows and 6 columns. And the columns are printed. For the columns, GEO_ID is the unqiue identifier for a geographic area and serves as the primary key for joining with other datasets. ZCTAS is the Zip Code Tabulation Area, NAME is the name typically associated with the zip. LSAD stands for Legal/Statistical Area Description which indicates the type of area the zip represents (so incorported, unincorporated, etc). CENSUSAREA represents the area size of the zip code region, usually measured in miles or kilometers and provides informations about the geographic extent of the area. The geometry column contains the geometric representation of the zip code area, holding the acutal shape and boundaries of the zip as a geometric object.

```{python}
#zips_all = gpd.read_file('/Users/justinesilverstein/Desktop/problem-set-4-andrew-justine/gz_2010_us_860_00_500k.shp')

zips_all = gpd.read_file("C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine")

zips_all_centroids = zips_all.copy()
zips_all_centroids['geometry'] = zips_all_centroids.geometry.centroid

print(zips_all_centroids.shape)
print(zips_all_centroids.columns) 
```

2. Create two GeoDataFrames as subsets of zips_all_centroids. First,
create all zip codes in Texas: zips_texas_centroids. Then, create all zip codes in Texas or a bordering state: zips_texas_borderstates_centroids, using the zip code prefixes to make these subsets. How many unique zip codes are in each of these subsets? 

There are 402 unique zip codes in Texas and 1314 unique zip codes in the bordering states.

```{python}
zips_all_centroids = gpd.read_file("C:/Users/andre/Documents/GitHub/problem-set-4-andrew-justine/gz_2010_us_860_00_500k.shp")
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].str.startswith('75')]

unique_texas_zip_codes = zips_texas_centroids['ZCTA5'].nunique()
print(f"Unique ZIP codes in Texas: {unique_texas_zip_codes}")

bordering_states_prefixes = ['87', '73', '72', '70']
zips_texas_borderstates_centroids = zips_all_centroids[
    zips_all_centroids['ZCTA5'].str.startswith(tuple(bordering_states_prefixes))
]

unique_bordering_zip_codes = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print(f"Unique ZIP codes in Texas and bordering states: {unique_bordering_zip_codes}")
```

3. Then create a subset of zips_texas_borderstates_centroids that contains only the zip codes with at least 1 hospital in 2016. Call the resulting GeoDataFrame zips_withhospital_centroids What kind of merge did you decide to do, and what variable are you merging on?

I first changed the column names to match and then inner merged on the variable ZIP_CD. There are 148 Texas zip codes with at least one hospital.

```{python}
active_2016 = df_health[(df_health['PGM_TRMNTN_CD'] == 0) & (df_health['Year'] == 2016)]
zips_texas_borderstates_centroids = zips_texas_borderstates_centroids.rename(columns={'ZCTA5': 'ZIP_CD'})

hospital_zip_df = active_2016.rename(columns={'ZIP_CD': 'ZIP_CD'}) 
hospital_zip_df['ZIP_CD'] = hospital_zip_df['ZIP_CD'].astype(str)
zips_texas_borderstates_centroids['ZIP_CD'] = zips_texas_borderstates_centroids['ZIP_CD'].astype(str)

zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospital_zip_df, on='ZIP_CD', how='inner'
)

print(f'Number of ZIP codes with at least one hospital in 2016: {len(zips_withhospital_centroids)}')
```

4. For each zip code in zips_texas_centroids, calculate the distance to the
nearest zip code with at least one hospital in zips_withhospital_centroids.

    a.This is a computationally-intensive join. Before attempting to do the entire join, subset to 10 zip codes in zips_texas_centroids and try the join. How long did it take? Approximately how long do you estimate the entire procedure will take?

    The code for this question is above. It took 9.98 seconds to compute for 10 zip codes. I think it will take roughly 150 seconds for the entire procedure to complete. 

```{python}
import time

def calculate_nearest_hospital_distance(zips, hospitals):
    zips = zips.to_crs(hospitals.crs)
    distances = zips.geometry.apply(
        lambda zip_geom: hospitals.distance(zip_geom).min()
    )
    return distances

subset_texas_zips = zips_texas_centroids.sample(n=10, random_state=1) 
start_time = time.time()
subset_texas_zips['Nearest_Hospital_Distance'] = calculate_nearest_hospital_distance(subset_texas_zips, zips_withhospital_centroids)
end_time = time.time()

print(subset_texas_zips[['ZCTA5', 'Nearest_Hospital_Distance']])
print(f'Time taken for subset calculation: {end_time - start_time:.4f} seconds')
```

    b. Now try doing the full calculation and time how long it takes. How close is it to your estimation?

    The code for this question is above. It took 437.47 seconds to do the whole calculation, which was very far from my estimate.

```{python}
import time

def calculate_nearest_hospital_distance(zips, hospitals):
    zips = zips.to_crs(hospitals.crs)
    distances = zips.geometry.apply(
        lambda zip_geom: hospitals.distance(zip_geom).min()
    )
    return distances

start_time = time.time()
zips_texas_centroids['Nearest_Hospital_Distance'] = calculate_nearest_hospital_distance(zips_texas_centroids, zips_withhospital_centroids)
end_time = time.time()

print(zips_texas_centroids[['ZCTA5', 'Nearest_Hospital_Distance']].head())
print(f'Time taken for full calculation: {end_time - start_time:.4f} seconds')
```

5. Calculate the average distance to the nearest hospital for each zip code in Texas.

    a. What unit is this in?
    Since we are using geometry distance, it is originally in units or degrees which are not useful for our purposes to gauge distance.

 ```{python}
# Convert the nearest hospital distances from meters to miles
zips_texas_centroids['Nearest_Hospital_Distance_Miles'] = zips_texas_centroids['Nearest_Hospital_Distance']

average_distance_miles = zips_texas_centroids['Nearest_Hospital_Distance_Miles'].mean()

print(f"Average distance to the nearest hospital for each ZIP code in Texas: {average_distance_miles:.2f} miles")
```

    b. Report the average distance in miles. Does this value make sense?

    This value does make sense as all distance in the US is measured in miles, units is not useful to gauge real life distance. After converting to miles, the average is 1.4 miles to the nearest hospital.

```{python}
# Convert the nearest hospital distances from meters to miles
zips_texas_centroids['Nearest_Hospital_Distance_Miles'] = zips_texas_centroids['Nearest_Hospital_Distance']

average_distance_miles = zips_texas_centroids['Nearest_Hospital_Distance_Miles'].mean()

print(f"Average distance to the nearest hospital for each ZIP code in Texas: {average_distance_miles:.2f} miles")
```

    c. Map the value for each zip code.

```{python}
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(12, 10))
zips_texas_centroids.plot(column='Nearest_Hospital_Distance_Miles',
                          ax=ax,
                          legend=True,
                          cmap='OrRd',
                          edgecolor='black',
                          legend_kwds={'label': "Distance to Nearest Hospital (Miles)",
                                       'orientation': "horizontal"})

ax.set_title('Distance to Nearest Hospital for Texas ZIP Codes', fontsize=15)
ax.set_xlabel('Longitude', fontsize=12)
ax.set_ylabel('Latitude', fontsize=12)

plt.show()
```    

## Effects of closures on access in Texas (15 pts)

1. 
Create df with only "cleaned" closures
```{python}
#merge to bring in zipcodes, but only those caught in total_closures
new_total_closures = pd.merge(total_closures, suspected_closures, on = "FAC_NAME", how = "inner")

#drop duplicates
new_total_closures = new_total_closures.drop_duplicates(subset = "FAC_NAME")

#drop "Year_y"
new_total_closures = new_total_closures.drop(columns = "Year_y")

#change Year_x to Year
new_total_closures = new_total_closures.rename(columns = {"Year_x":"Year"})

```

Use groupby to create list of closures
```{python}
#groupby zipcode
TX_closed = new_total_closures.groupby("ZIP_CD")["FAC_NAME"].count()

#reset index
TX_closed = TX_closed.reset_index()

#to DataFrame
TX_closed = pd.DataFrame(TX_closed)

#rename FAC_NAME to Closure_Count
TX_closed = TX_closed.rename(columns = {"FAC_NAME":"Closure_Count"})
```

Convert ZIP_CD to string
```{python}
#convert ZIP_CD to float
TX_closed["ZIP_CD"] = TX_closed["ZIP_CD"].astype(float)
#convert ZIP_CD to int
TX_closed["ZIP_CD"] = TX_closed["ZIP_CD"].apply(np.int64)
#convert ZIP_CD to string
TX_closed["ZIP_CD"] = TX_closed["ZIP_CD"].astype(str)

#add in_Texas column
in_Texas(TX_closed, "ZIP_CD")
#filter for only Texas
TX_closed = TX_closed[TX_closed["in_Texas"] == 1]

print("This table shows hospital closures in TX by zipcode")

print(TX_closed)
```



2. 
Perform a merge to create shapefile containing the closure_count data
```{python}

shape_closure = pd.merge(Tex_shape, TX_closed, left_on = "ZCTA5", right_on = "ZIP_CD", how = "left")

#fill in NA values in Closure_Count with 0
shape_closure["Closure_Count"] = shape_closure["Closure_Count"].fillna(0)

#drop ZIP_CD (leaving only ZCTA5) and drop in_Texas_y

shape_closure = shape_closure.drop(columns = "ZIP_CD")

shape_closure = shape_closure.drop(columns = "in_Texas_y")

#rename in_Texas_x to in_Texas

shape_closure = shape_closure.rename(columns = {"in_Texas_x":"in_Texas"})

```

Plot choropleth
```{python}
shape_closure.plot(column = "Closure_Count", legend = True)


#also print number of directly affected zipcodes
len(shape_closure[shape_closure["Closure_Count"] >= 1])
```

It appears that there are 231 zipcodes in Texas that were
directly impacted by a closure, based off of the shape file
data we created. However, this is about 20 zipcodes lower
than the number implied in the list of TX closures by zipcode.
There are a number of potential reasons for this, including 
potential error in the original data (e.g. zipcodes that 
are not included in the .shp file) or, perhaps more likely,
a consequence of data cleaning choices. 

Attribution: I asked ChatGPT why I might have lost some values
in using pd.merge()
3. 

```{python}


#create geodataframe subsetted to directed codes
sub_closure = shape_closure[shape_closure["Closure_Count"] >= 1]

#
sub_closure.centroid.buffer(1/10).plot(edgecolor="white").set_axis_off()
plt.axis("off")
plt.show()



```

4. 

## Reflecting on the exercise (10 pts) 


1. Based on my understanding of the methodology, I can see
a few different ways in which our first pass may be imperfect.
Firstly, it is possible that, due to some of the complexities 
involved in hospital ownership, some hospitals that we may 
mark as open may have changed their terms of service, and no 
longer fall under the category of short term hospital, but 
not have reported this at the time that they reported their 
status to the relevant authorities. Additionally, some of the
hospital names (FAC_NAME) actually have the word closed in them,
for multiple years, and it would probably be worthwhile to 
select for those hospital names to understand how many there are,
and potentially create a comparison between the hospitals that 
are reported as closed in the dataset and the list of hospitals 
that we identified as closed in the first pass. 

We could also cross-reference data from other sources to develop
multiple indicators determining whether a hospital is open or 
closed, and then create a measure with multiple levels of 
confidence (e.g. if the hospital is listed as closed by local
property taxation records and in this dataset, it will be at 
level 2, and so on. The objective would be to see if the hospital
matches status across data sets.) 

2. Consider the way we are identifying zip codes affected by closures. How well does this reflect changes in zip-code-level access to hospitals? Can you think of some ways to improve this measure?

I think there are issues with how we code and define true closures from mergers/acquisitions. The definition itself is confusng as we compare different observations from a year to year basis but a hospital could still close even if it was a merger. Furthermore, it is unclear how we discuss what we count as a merger, especially if it gets confusing how to calculate when a merger occurred. Also, this does not take into account urban areas vs rural area where distance makes a huge difference for people. I do not think mergers and acquisitions reflect changes in zip code level hospital access siginifcantly. I think it the type of facility closure is more likely to have an impact.

